=pod

=encoding utf-8

=head1 Como fazer um crawler ineficiente

=over

=item *

Delegue a I<normalização> dos links ao método C<canonical()> do L<URI>.
E daí que o C<path> pode ter coisas como C<../../../>, ou parâmetros da I<query> podem vir em ordens distintas?

=item *

Compile L<libcurl|http://curl.haxx.se/> com suporte a L<c-ares|http://c-ares.haxx.se/> sempre que o seu crawler for de tipo horizontal (pegar muitas páginas do mesmo servidor).

=item *

Distribua o seu crawler horizontal em maior número de nodes possível.

=item *

Sempre use L<WWW::Mechanize>.
Jamais se importe com C<stack_depth>.

=item *

Use e abuse do L<HTML::TreeBuilder::XPath>.
L<Web::Scraper::LibXML> é para os fracos!

=item *

Melhor ainda: L<processe HTML usando expressões regulares|http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454>

=item *

Alcançando o nirvana: baixe os documentos usando C<wget -r -np> e os processe com expressões regulares!

=item *

Use L<fork> para criar conexões paralelas.
E L<DBD::SQLite> para gerenciá-las.
Todos sabem que, quanto mais conexões, mais rápido o download!
Comece com um número razoável, por exemplo 100.

=item *

Num sistema SMP, nada melhor do que I<threads> para distribuir as tarefas entre as CPUs.
E, no Windows, I<ithreads> garantem a melhor escalabilidade para o seu crawler.

=item *

Repudie L<eval>, L<Try::Tiny> e afins.
É impossível o crawler gerar I<exception> ao percorrer sites alheios.
Corolario: ignore a questão de I<retries>, afinal, tudo vai funcionar perfeitamente logo na primeira tentativa!

=item *

Guarde todos os resultados crawleados na RAM.
Grave-os no storage permanente somente no final da execução.

=item *

Nem sempre salve os dados obtidos.
Mas, quando salvar, certifique-se de gerar um arquivo com instruções SQL, para carregar da forma mais segura possível na sua base de dados.
Ou, melhor ainda, grave um XML (cuidado para não cair na armadilha de usar L<XML::Compile>!), empregando C<CDATA> para I<escape>.

=item *

Sabe aquele negócio chato de C<site.com.br>/C<www.site.com.br>?
Resolva com um simples C<s/^www\.//>!

=item *

Lembre-se: a única codificação que existe é ISO-5589-1.
Se você se deparar com caracteres UTF-8, não hesite em usar C<s///g> para convertê-los na única codificação que é verdadeira!

=item *

Ignore F<robots.txt> e, especialmente, F<sitemap.xml>.

=item *

Com certeza, você sabe parsear headers (inclusive cookies) muito melhor do que os idiotas que submeteram os respectivos módulos para o CPAN.
Não perca tempo com essas dependências triviais.

=item *

Não esquente a cabeça com a condição de parada do crawler.
Uma hora ou outra, essa bagaça tem que parar!

=item *

Considere a beleza inerente da solução recursiva.
Por um instante, esqueça de que o Perl é, na sua essência, imperativo.

=item *

Preocupe-se com a beleza do código do seu crawler.
Crawler bom é crawler I<clean>.
Utilize o L<LWP::Simple>, ou, melhor ainda: implemente em Ruby.

=item *

Tenha em mente que L<Scrapy|http://scrapy.org/>, L<Nutch|http://nutch.apache.org/>, L<Methabot|http://sourceforge.net/projects/methabot/>, L<Heritrix|https://webarchive.jira.com/wiki/display/Heritrix/Heritrix> e afins são tudo uns lixos.
Certamente, o seu crawler ficará muito melhor!

=item *

Siga todas essas dicas à risca, ou o seu crawler correrá o risco de ficar eficiente!

=back

=head2 Referências

Ao fazer um crawler ineficiente, ignore a opinião dos seguintes elementos:

=over

=item *

L<@creaktive|https://github.com/creaktive>

=item *

L<@fvox|https://github.com/fvox>

=item *

L<@jimmytty|https://github.com/jimmytty>

=item *

L<@mantovani|https://github.com/mantovani>

=item *

L<@renatocron|https://github.com/renatocron>

=back
