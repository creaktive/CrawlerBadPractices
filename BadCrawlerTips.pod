=pod

=encoding utf-8

=head1 Como fazer um crawler ineficiente

=over

=item *

Delegue a I<normalização> dos links ao método C<canonical()> do L<URI>.
E daí que o C<path> pode ter coisas como C<../../../>, ou parâmetros da I<query> podem vir em ordens distintas?

=item *

Compile L<libcurl|http://curl.haxx.se/> com suporte a L<c-ares|http://c-ares.haxx.se/> sempre que o seu crawler for de tipo horizontal (pegar muitas páginas do mesmo servidor).

=item *

Distribua o seu crawler horizontal em maior número de nodes possível.

=item *

Sempre use L<WWW::Mechanize>.
Jamais se importe com C<stack_depth>.

=item *

Use e abuse do L<HTML::TreeBuilder::XPath>.
L<Web::Scraper::LibXML> é para os fracos!

=item *

Melhor ainda: L<processe HTML usando expressões regulares|http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454>

=item *

Alcançando o nirvana: baixe os documentos usando C<wget -r -np> e os processe com expressões regulares!

=item *

Use L<fork> para criar conexões paralelas.
E L<DBD::SQLite> para gerenciá-las.
Todos sabem que, quanto mais conexões, mais rápido o download!
Comece com um número razoável, por exemplo 100.

=item *

Guarde todos os resultados crawleados na RAM.
Grave-os no storage permanente somente no final da execução.

=item *

Sabe aquele negócio chato de C<site.com.br>/C<www.site.com.br>?
Resolva com um simples C<s/^www\.//>!

=item *

Lembre-se: a única codificação que existe é ISO-5589-1.
Se você se deparar com caracteres UTF-8, não hesite em usar C<s///g> para convertê-los na única codificação que é certa!

=item *

Ignore F<robots.txt> e, especialmente, F<sitemap.xml>.

=item *

Com certeza, você sabe parsear headers (inclusive cookies) muito melhor do que os idiotas que submeteram os respectivos módulos para o CPAN.
Não perca tempo com essas dependências triviais.

=item *

Não esquente a cabeça com a condição de parada do crawler.
Uma hora ou outra, essa bagaça tem que parar!

=item *

Considere a beleza inerente da solução recursiva.
Por um instante, esqueça de que o Perl é, na sua essência, imperativo.

=item *

Preocupe-se com a beleza do código do seu crawler.
Crawler bom é crawler I<clean>.
Utilize o L<LWP::Simple>, ou, melhor ainda: implemente em Ruby.

=item *

Tenha em mente que L<Scrapy|http://scrapy.org/>, L<Nutch|http://nutch.apache.org/>, L<Methabot|http://sourceforge.net/projects/methabot/>, L<Heritrix|https://webarchive.jira.com/wiki/display/Heritrix/Heritrix> e afins são tudo uns lixos.
Certamente, o seu crawler ficará muito melhor!

=item *

Siga todas essas dicas à risca, ou o seu crawler correrá o risco de ficar eficiente!

=back